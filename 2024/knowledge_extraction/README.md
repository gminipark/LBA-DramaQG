# LBA - Knowledge Extraction (Text2Data) 2024 version

## Setting
### 1. Clone Repo
```bash
git clone https://github.com/gminipark/LBA-DramaQG.git
```
### 2. Install Dependencies
ì½”ë“œ ì‹¤í–‰ì— í•„ìš”í•œ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜
```bash
$ pip install -r requirements.txt
```
### 3. Directory Structure
ì•„ëž˜ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ êµ¬ì¶•
(Dataì— í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ëŠ” ì§ì ‘ ìƒì„±í•´ì•¼í•¨)
```bash
.
â””â”€â”€ LBA-DramaQG
    â”œâ”€â”€ 2022
    â”œâ”€â”€ 2023
    â””â”€â”€ 2024
        â”œâ”€â”€ knowledge_extraction
        â”‚Â Â  â”œâ”€â”€ Data
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ DramaQA_KG
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ DramaQA_KG_Processed
        â”‚Â Â  â”‚Â Â  â””â”€â”€ FewShotDemo
        â”‚Â Â  â”œâ”€â”€ LLaMA_3_1
        â”‚Â Â  â”‚Â Â  â””â”€â”€ fine_tuning
        â”‚Â Â  â””â”€â”€ Test
        â””â”€â”€ ...
```
### 4. Dataset Save
[íŒŒì¼ ì €ìž¥ëœ ê¹ƒí—™ ë§í¬](https://github.com/wjcldply/LBA-Text2Data-Public.git)ì—ì„œ `Data` ë””ë ‰í† ë¦¬ ë‚´ì˜ `DramaQA_KG`, `FewShotDemo` íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë™ì¼ ê²½ë¡œì— ì €ìž¥í•´ì•¼ í•¨

### 5. SETUP & LOGIN
- (í„°ë¯¸ë„ ë‚´ì—ì„œ) HFðŸ¤— & WANDB & OpenAI ë¡œê·¸ì¸

## Training / Testing

### Data-Building
- Text2Data ë°ì´í„°ì…‹ / ìŠ¤í‚¤ë§ˆì…‹ ìƒì„±
    ```bash
    $ python -m LLaMA3_1.Test.kg_json_rows_prep.py
    ```
- í…ŒìŠ¤íŠ¸ìš© ëžœë¤ìƒ˜í”Œ ìƒì„±
    ```bash
    $ python -m LLaMA3_1.Test.json_random_picker.py
    ```
- Fine-Tuning Dataset ìƒì„±
    ```bash
    $ python -m LLaMA3_1.fine_tuning.train_data_build
    ```

### Fine-Tuning
```bash
$ export RunID=WANDB_RUNID_TO_USE

$ nohup python -u -m LLaMA3_1.fine_tuning.lora_train_llama \
    --dataset_text_field Text_ZeroShot \
    --base_model_path meta-llama/Meta-Llama-3.1-8B-Instruct \
    --new_model_name LLaMA-3.1-8B-LBA-LoRA-ZeroShot\
    --context_window 2048 \
    --lora_rank 32 \
    --lora_alpha 16 \
    --epochs 5 \
    --per_device_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --checkpointing_ratio 0.2 \
    --fp16 \
    --wandb_run_name $RunID \
    > ./Logs/$RunID.log 2>&1 &

$ nohup python -u -m LLaMA3_1.fine_tuning.lora_train_llama.py > ./Logs/FineTuning.log 2>&1 &
```

### Inference (BackBone: HF Transformers; LLaMA3.1-8B)
```bash
$ python -m Test.eval_week9.py
```